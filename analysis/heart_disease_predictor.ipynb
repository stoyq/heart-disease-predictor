{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6feac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d53971-c347-4a75-a7e3-f741cf379e56",
   "metadata": {},
   "source": [
    "## DSCI 522 – Milestone 1 - Group 25 \n",
    "### Project Name: Heart Disease Prediction Model\n",
    "\n",
    "### Team Members:\n",
    "#### Johnson Chuang | Eduardo Sanches | Azadeh Ramesh | Jose Davila\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e0750-d37d-4691-ab8e-d9565c0d12fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Data analysis and workflow project for DSCI 522 (Data Science Workflows), a course in the Master of Data Science program at the University of British Columbia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd39be-dbc9-4d5e-8d22-22f1719f0063",
   "metadata": {},
   "source": [
    "#### **GitHub link:** https://github.com/stoyq/heart-disease-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312c70a-cb55-4f96-a475-ee2c87b698d7",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Heart disease is one of the leading causes of death globally, and early detection is critical for prevention and treatment. In this project, we use the UCI Heart Disease dataset to build a machine-learning model that predicts whether a patient is likely to have heart disease based on clinical and physiological attributes. We load the dataset directly from the web, clean and wrangle the data, perform exploratory data analysis (EDA), and train a classification model (Decision Tree) to identify important predictors of heart disease. Our results highlight key risk indicators that align with well-known medical knowledge, demonstrating how machine learning can support early screening and clinical decision-making.\n",
    "\n",
    "### Introduction \n",
    "The objective of this project is to develop a predictive model that determines whether a patient is at risk of heart disease using a set of clinical measurements. Heart disease diagnoses often rely on many interacting factors such as chest pain symptoms, blood pressure, cholesterol levels, and exercise response. Machine-learning models can help uncover patterns in these variables and support early identification of high-risk patients.\n",
    "\n",
    "Our research question is:\n",
    "\n",
    "“Given a patient’s clinical and physiological attributes, can we accurately predict whether they have heart disease?”\n",
    "\n",
    "To answer this question, we use the publicly available Heart Disease dataset from the UCI Machine Learning Repository. This dataset contains multiple medically relevant variables, making it suitable for a classification model such as a Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad4ea4-5982-4bf3-9791-053e72da39b5",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "We use the Heart Disease dataset from the UCI Machine Learning Repository, a widely used benchmark dataset for medical prediction tasks. The dataset includes the following 14 attributes:\n",
    "- Age\n",
    "- Sex\n",
    "- Chest Pain Type (cp)\n",
    "- Resting Blood Pressure (trestbps)\n",
    "- Cholesterol (chol)\n",
    "- Fasting Blood Sugar (fbs)\n",
    "- Resting ECG results (restecg)\n",
    "- Maximum heart rate achieved (thalach)\n",
    "- Exercise induced angina (exang)\n",
    "- ST depression (oldpeak)\n",
    "- Slope of ST segment (slope)\n",
    "- Number of major vessels (ca)\n",
    "- Thalassemia result (thal)\n",
    "- num (Target: the predicted attribute (0 = no heart disease, 1 = heart disease))\n",
    "\n",
    "These variables include both continuous and categorical measurements commonly used in clinical diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4313f-935b-45c3-ae83-97dca430f5f5",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "We build a machine-learning classification model using the UCI Heart Disease dataset:\n",
    "\n",
    "1. Load data from the original source on the web: https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "\n",
    "2. Wrangle and clean the data\n",
    "\n",
    "- Replace missing values\n",
    "- Assign meaningful column names\n",
    "- Convert categorical variables to numeric where needed\n",
    "- Ensure that the target variable is binary (0 = no heart disease, 1 = heart disease)\n",
    "\n",
    "3. Perform exploratory data analysis (EDA)\n",
    "- Summary statistics for continuous variables\n",
    "- Count plots for categorical variables\n",
    "- Histograms and boxplots to understand feature distributions\n",
    "\n",
    "4. Create visualizations relevant to the classification task\n",
    "- Pairplots to explore relationships between key features\n",
    "- Distribution of target classes\n",
    "- Feature correlation matrix\n",
    "\n",
    "5. Build a classification model\n",
    "- A Decision Tree Classifier is trained to predict heart disease.\n",
    "- We split the dataset into training and testing subsets and evaluate model accuracy.\n",
    "\n",
    "6. Visualize the model results\n",
    "- Plot of the trained Decision Tree\n",
    "- Feature importance bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68304431-3792-436b-a55b-5b346d27a4c0",
   "metadata": {},
   "source": [
    "### Importing the Dataset\n",
    "\n",
    "A special note about our data download process: The following code downloads the zip file from UCI's website, unpacks them, and grabs the data of interest (Cleveland data). It is then processed minimally by adding the correct column names, and finally written out as a CSV to the data/processed folder.\n",
    "\n",
    "In our actual analysis, we fetch the same data directly using UCI's own `ucimlrepo` library. The data is the same. But we include this part to show how you can download the data without UCI's own library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938557bd-7eb0-4d01-988e-e779c756037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete! File saved to data/raw/processed.cleveland.data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# This is the URL to the data. There are many files in the zip file\n",
    "# In particular we will retrieve the cleveland data\n",
    "url = \"https://archive.ics.uci.edu/static/public/45/heart+disease.zip\"\n",
    "\n",
    "# Make sure the proper data folders exist\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Download the zip file into memory\n",
    "response = requests.get(url)\n",
    "\n",
    "# Open the zip from memory\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    # We only want the Cleveland data\n",
    "    z.extract(\"processed.cleveland.data\", \"../data/raw\")\n",
    "\n",
    "print(\"Download complete! File saved to data/raw/processed.cleveland.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56e9c63-e12b-40ee-8191-aafaee626f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/processed.cleveland.data\", header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0539f35f-cc27-41e6-9e89-d3781ce7080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       "\n",
       "     slope   ca thal  target  \n",
       "0      3.0  0.0  6.0       0  \n",
       "1      2.0  3.0  3.0       2  \n",
       "2      2.0  2.0  7.0       1  \n",
       "3      3.0  0.0  3.0       0  \n",
       "4      1.0  0.0  3.0       0  \n",
       "..     ...  ...  ...     ...  \n",
       "298    2.0  0.0  7.0       1  \n",
       "299    2.0  2.0  7.0       2  \n",
       "300    2.0  1.0  7.0       3  \n",
       "301    2.0  1.0  3.0       1  \n",
       "302    1.0    ?  3.0       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de125a32-015c-433e-b4c6-49158c092e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write complete! File saved to data/processed/cleveland_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Write out processed data\n",
    "df.to_csv(\"../data/processed/cleveland_clean.csv\", index=False)\n",
    "print(\"Write complete! File saved to data/processed/cleveland_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef138c8-e41c-40d7-aef7-786998c98ff1",
   "metadata": {},
   "source": [
    "### Importing the Dataset Using `ucimlrepo`\n",
    "\n",
    "As mentioned in the above section, we are fetching the same data. This time we are using the `ucimlrepo` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d0923b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ucimlrepo'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mucimlrepo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_ucirepo \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# fetch dataset \u001b[39;00m\n\u001b[32m      4\u001b[39m heart_disease = fetch_ucirepo(\u001b[38;5;28mid\u001b[39m=\u001b[32m45\u001b[39m) \n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ucimlrepo'"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "  \n",
    "# Debug metadata (uncomment to see)\n",
    "#print(heart_disease.metadata) \n",
    "  \n",
    "# Debug variable information (uncomment to see)\n",
    "#print(heart_disease.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81e585",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce88263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mX\u001b[49m.head(\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge X and y in df for EDA\n",
    "df = X.copy()\n",
    "df[\"target\"] = y\n",
    "\n",
    "def plot_overlap(feature):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df[df.target == 0][feature], bins = 20, alpha = 0.6, label =\"No Disease\")\n",
    "    plt.hist(df[df.target == 1][feature], bins = 20,alpha = 0.6, label = \"Disease\")\n",
    "    plt.title(f\"Distribution of {feature} by Heart Disease Status\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_overlap(\"age\")\n",
    "plot_overlap(\"chol\")\n",
    "plot_overlap(\"trestbps\")\n",
    "plot_overlap(\"thalach\")\n",
    "plot_overlap(\"oldpeak\")\n",
    "plot_overlap(\"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a8e62",
   "metadata": {},
   "source": [
    "### Column Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aef765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "numerical = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "categorical = [\"cp\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "binary = [\"sex\", \"fbs\", \"exang\"]\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler() , numerical),\n",
    "    (OneHotEncoder(handle_unknown = \"ignore\"), categorical),\n",
    "    (\"passthrough\", binary)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933768be",
   "metadata": {},
   "source": [
    "### Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "numeric = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "categorical = [\"cp\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "binary = [\"sex\", \"fbs\", \"exang\"]\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler() , numeric),\n",
    "    (OneHotEncoder( handle_unknown = \"ignore\"), categorical),\n",
    "    (\"passthrough\", binary)\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size =0.2, random_state = 123\n",
    ")\n",
    "\n",
    "svc_pipe = make_pipeline( preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818f115",
   "metadata": {},
   "source": [
    "### Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad16903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "cross_val_results = {}\n",
    "cross_val_results['SVC'] = pd.DataFrame(cross_validate(svc_pipe, X_train, y_train, cv =5, return_train_score= True)).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "\n",
    "cross_val_results['SVC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac690dc",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8081f3",
   "metadata": {},
   "source": [
    "### Predict (X_test) and compatr with Actuals (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c281fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comparison = pd.DataFrame()\n",
    "comparison[\"Predictions\"] = svc_pipe.predict(X_test)\n",
    "comparison[\"Actual\"] = y_test.values \n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dea43f",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "The Decision Tree model was able to identify meaningful patterns to predict heart disease based on the data, with a test score of 0.61 and train score of 0.78. Based on these results, it might indicate that there was some overfitting based on the large difference between training and test results.\n",
    "\n",
    "From the EDA, we see that various features such as age, sex, chol and more have clear differences in their distribution between disease and no disease which will help the model to predict between the two. For a better predictor, we may want to incorporate additional features given the complexity of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a8db6-dc6d-4181-928a-2b74e0fb2e88",
   "metadata": {},
   "source": [
    "### Results and Conclusion\n",
    "\n",
    "Our analysis shows that several clinical features differ noticeably between patients with and without heart disease. As seen in the EDA histograms, patients with heart disease tend to have higher resting blood pressure (trestbps), higher ST-depression values (oldpeak), and lower maximum heart rate achieved (thalach) compared to individuals without disease. After preprocessing the dataset using scaling for numerical variables and one-hot encoding for categorical variables, we trained a Support Vector Classifier (SVC) model. Cross-validation results indicate an average test accuracy of 0.61, with a higher training accuracy of 0.78, suggesting some overfitting. When evaluating predictions on the unseen test set, the model correctly identified many cases but also showed several misclassifications, especially where the model predicted “0” (no disease) but the true label was “1” or “2.” Overall, while the model captures meaningful patterns in the dataset, its moderate predictive performance suggests that further tuning, alternative models, or feature engineering may be needed to improve accuracy and reduce classification bias.\n",
    "\n",
    "### References\n",
    "- UCI Machine Learning Repository. Heart Disease Dataset: https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "- International application of a new probability algorithm for the diagnosis of coronary artery disease. By R. Detrano, A. Jánosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher. 1989 Published in American Journal of Cardiology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6341df-cd29-4015-a287-bfc31e49843f",
   "metadata": {},
   "source": [
    "### Data Validation Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6e927-4ace-4027-bb0f-b0eb1297a6eb",
   "metadata": {},
   "source": [
    "### Step 1. Data Types Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d103d-7b52-429e-88fc-290136183465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation -> Step 1: Data Types Check\n",
    "\n",
    "expected_types = {\n",
    "    \"age\": \"int64\",\n",
    "    \"sex\": \"int64\",\n",
    "    \"cp\": \"int64\",\n",
    "    \"trestbps\": \"int64\",\n",
    "    \"chol\": \"int64\",\n",
    "    \"fbs\": \"int64\",\n",
    "    \"restecg\": \"int64\",\n",
    "    \"thalach\": \"int64\",\n",
    "    \"exang\": \"int64\",\n",
    "    \"oldpeak\": \"float64\",\n",
    "    \"slope\": \"int64\",\n",
    "    \"ca\": \"float64\",\n",
    "    \"thal\": \"int64\",\n",
    "    \"target\": \"int64\"\n",
    "}\n",
    "\n",
    "type_errors = []\n",
    "\n",
    "for col, expected in expected_types.items():\n",
    "    if str(df[col].dtype) != expected:\n",
    "        type_errors.append(f\"Column '{col}' has incorrect type: {df[col].dtype} (expected {expected})\")\n",
    "\n",
    "if type_errors:\n",
    "    print(\"Data Type Errors Found:\")\n",
    "    for e in type_errors:\n",
    "        print(\"-\", e)\n",
    "else:\n",
    "    print(\"All data types are correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6487e2-03e8-481b-a1f7-2274ec4ddfb2",
   "metadata": {},
   "source": [
    "### Step 2. Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fd1c64-25de-42c6-bba2-893e90fc0fd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m missing = \u001b[43mdf\u001b[49m.isna().sum()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMissing values per column:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(missing)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Validation -> Step 2: Missing Values Check\n",
    "\n",
    "missing = df.isna().sum()\n",
    "\n",
    "print(\"Missing values per column:\\n\")\n",
    "print(missing)\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\n No missing values detected.\")\n",
    "else:\n",
    "    print(\"\\n Missing values found. Please investigate before modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a888357-ef3c-491b-b7fe-d0c8711ffa08",
   "metadata": {},
   "source": [
    "### Step 3. Duplicate Rows Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287a4372-8cb5-493f-9d7e-2fe2f83e36ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3173464293.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThis step:\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Data Validation -> Step 3: Duplicate Rows Check\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count == 0:\n",
    "    print(\"No duplicate rows detected.\")\n",
    "else:\n",
    "    print(\"Duplicate rows found. Please review and remove them before modeling.\")\n",
    "This step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad31030-4a43-4ae6-954a-5ade78508b7e",
   "metadata": {},
   "source": [
    "### Step 4. Category Levels Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d53cf-7659-4057-b528-a819679b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation Check -> Step 4: Category Levels\n",
    "\n",
    "# Expected allowed values for categorical fields\n",
    "expected_categories = {\n",
    "    \"sex\": [0, 1],\n",
    "    \"cp\": [1, 2, 3, 4],\n",
    "    \"fbs\": [0, 1],\n",
    "    \"restecg\": [0, 1, 2],\n",
    "    \"exang\": [0, 1],\n",
    "    \"slope\": [1, 2, 3],\n",
    "    \"ca\": [0, 1, 2, 3, 4],   \n",
    "    \"thal\": [3, 6, 7],       \n",
    "    \"target\": [0, 1]         \n",
    "}\n",
    "\n",
    "category_errors = []\n",
    "\n",
    "for col, allowed in expected_categories.items():\n",
    "    if col in df.columns:\n",
    "        invalid_values = set(df[col].unique()) - set(allowed)\n",
    "        if invalid_values:\n",
    "            category_errors.append(f\"Column '{col}' contains invalid values: {invalid_values}\")\n",
    "\n",
    "if category_errors:\n",
    "    print(\"Category Level Errors Found:\")\n",
    "    for e in category_errors:\n",
    "        print(\"-\", e)\n",
    "else:\n",
    "    print(\"All categorical columns contain valid allowed values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396e509-4880-4d0b-b4f5-584b69009326",
   "metadata": {},
   "source": [
    "### Step 5. Logical Ranges Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382f4a41-d779-4c6f-8034-93df773d9672",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      6\u001b[39m valid_ranges = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1\u001b[39m, \u001b[32m120\u001b[39m),              \u001b[38;5;66;03m# Age of adults\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrestbps\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m50\u001b[39m, \u001b[32m300\u001b[39m),        \u001b[38;5;66;03m# Resting blood pressure\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moldpeak\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.0\u001b[39m, \u001b[32m10.0\u001b[39m),       \u001b[38;5;66;03m# ST depression induced by exercise\u001b[39;00m\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col, (min_val, max_val) \u001b[38;5;129;01min\u001b[39;00m valid_ranges.items():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.columns:\n\u001b[32m     16\u001b[39m         invalid_low = df[df[col] < min_val]\n\u001b[32m     17\u001b[39m         invalid_high = df[df[col] > max_val]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Validation -> Step 5: Logical Ranges Check\n",
    "\n",
    "range_errors = []\n",
    "\n",
    "# Define expected valid ranges for each column\n",
    "valid_ranges = {\n",
    "    \"age\": (1, 120),              # Age of adults\n",
    "    \"trestbps\": (50, 300),        # Resting blood pressure\n",
    "    \"chol\": (50, 700),            # Serum cholesterol\n",
    "    \"thalach\": (50, 250),         # Max heart rate\n",
    "    \"oldpeak\": (0.0, 10.0),       # ST depression induced by exercise\n",
    "}\n",
    "\n",
    "for col, (min_val, max_val) in valid_ranges.items():\n",
    "    if col in df.columns:\n",
    "        invalid_low = df[df[col] < min_val]\n",
    "        invalid_high = df[df[col] > max_val]\n",
    "        \n",
    "        if not invalid_low.empty:\n",
    "            range_errors.append(f\"Column '{col}' has values below {min_val}. Examples: {invalid_low[col].tolist()[:5]}\")\n",
    "        if not invalid_high.empty:\n",
    "            range_errors.append(f\"Column '{col}' has values above {max_val}. Examples: {invalid_high[col].tolist()[:5]}\")\n",
    "\n",
    "# Special logical relationships\n",
    "# max heart rate must be >= 50\n",
    "if (df[\"thalach\"] < df[\"trestbps\"]).any():\n",
    "    range_errors.append(\"Some rows have thalach < trestbps, which is physiologically unlikely.\")\n",
    "\n",
    "if (df[\"oldpeak\"] < 0).any():\n",
    "    range_errors.append(\"oldpeak contains negative values, which are invalid.\")\n",
    "\n",
    "# Print results\n",
    "if range_errors:\n",
    "    print(\"Logical Range Errors Found:\")\n",
    "    for err in range_errors:\n",
    "        print(\"-\", err)\n",
    "else:\n",
    "    print(\"All numeric columns fall within expected logical / medical ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4288391-845b-4472-8e35-998c01670412",
   "metadata": {},
   "source": [
    "### Step 6: Train/Test Leakage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37961e0a-30b8-4d7c-9577-ebc286f022e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Split the data (same as used in your notebook)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X = \u001b[43mdf\u001b[49m.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m y = df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m     10\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m2024\u001b[39m, stratify=y\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Validation - > Step 6: Train/Test Leakage Check\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (same as used in your notebook)\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2024, stratify=y\n",
    ")\n",
    "\n",
    "leakage_errors = []\n",
    "\n",
    "# Check 1: No overlapping rows\n",
    "\n",
    "train_indices = set(X_train.index)\n",
    "test_indices = set(X_test.index)\n",
    "\n",
    "if train_indices & test_indices:\n",
    "    leakage_errors.append(\"Train and test sets have overlapping indices!\")\n",
    "\n",
    "# Check 2: No test data seen by encoders\n",
    "\n",
    "categorical_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    test_extra = set(X_test[col].unique()) - set(X_train[col].unique())\n",
    "    if test_extra:\n",
    "        leakage_errors.append(\n",
    "            f\"Column '{col}' has categories in TEST not present in TRAIN → potential leakage or mismatch: {test_extra}\"\n",
    "        )\n",
    "\n",
    "# Check 3: No target leakage into features\n",
    "\n",
    "if \"target\" in X_train.columns:\n",
    "    leakage_errors.append(\"Target column found in training features! Ensure drop(columns=['target']) is applied.\")\n",
    "\n",
    "# Print results\n",
    "if leakage_errors:\n",
    "    print(\"Data Leakage Detected:\")\n",
    "    for err in leakage_errors:\n",
    "        print(\"-\", err)\n",
    "else:\n",
    "    print(\"No data leakage detected. Train and test sets are fully independent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6983cb-5394-432b-a791-2f6be6e8e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
